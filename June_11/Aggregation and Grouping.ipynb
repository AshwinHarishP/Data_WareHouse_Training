{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7a85d49-cb09-4bee-b3d6-2f34fdc20496",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=1053308873284391#setting/sparkui/0611-085717-4ds1crvb/driver-1107440672652021086\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*, 4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x70a016dc5c10>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "                    .appName(\"Aggregation and Grouping\") \\\n",
    "                    .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74ea3fb4-32dc-4e65-bc77-1e4981eb393b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Dataset 1: Employee Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e0c5b4b-321f-4ffe-8cc8-777f3b070c3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [ \n",
    "        (\"Ananya\", \"HR\", 52000), \n",
    "        (\"Rahul\", \"Engineering\", 65000), \n",
    "        (\"Priya\", \"Engineering\", 60000), \n",
    "        (\"Zoya\", \"Marketing\", 48000), \n",
    "        (\"Karan\", \"HR\", 53000), \n",
    "        (\"Naveen\", \"Engineering\", 70000), \n",
    "        (\"Fatima\", \"Marketing\", 45000) \n",
    "        ] \n",
    "columns = [\"Name\", \"Department\", \"Salary\"]\n",
    "employee_df = spark.createDataFrame(data, columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "729ce0bf-183d-4df2-bf76-5155f975aa12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Dataset 2: Performance Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9eaa799-4848-4231-9dc0-c3327a759082",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "performance =[ \n",
    "                (\"Ananya\", 2023, 4.5), \n",
    "                (\"Rahul\", 2023, 4.9), \n",
    "                (\"Priya\", 2023, 4.3), \n",
    "                (\"Zoya\", 2023, 3.8), \n",
    "                (\"Karan\", 2023, 4.1), \n",
    "                (\"Naveen\", 2023, 4.7), \n",
    "                (\"Fatima\", 2023, 3.9) \n",
    "            ] \n",
    "columns_perf = [\"Name\", \"Year\", \"Rating\"] \n",
    "performance_df = spark.createDataFrame(performance, columns_perf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1117d433-7f8b-46aa-9691-828bb41a82c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## PySpark Exercises – Set 2 (Advanced)\n",
    "### GroupBy and Aggregations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a3e2d63-063e-4dac-865e-8a0ba09799ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Get the average salary by department."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f59b24f-6387-44c0-916e-337b221bf842",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+\n| Department|Average Salary|\n+-----------+--------------+\n|         HR|       52500.0|\n|Engineering|       65000.0|\n|  Marketing|       46500.0|\n+-----------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "employee_df.groupBy(employee_df.Department) \\\n",
    "           .agg(avg(employee_df.Salary).alias(\"Average Salary\")) \\\n",
    "           .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da88f37a-3fb5-4283-a15f-07f94928cdef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Count of employees per department."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3737ba18-8cdf-4163-aed3-639cd4b8aa66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+\n| Department|Number of Employees|\n+-----------+-------------------+\n|         HR|                  2|\n|Engineering|                  3|\n|  Marketing|                  2|\n+-----------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "employee_df.groupBy(employee_df.Department) \\\n",
    "           .count() \\\n",
    "           .withColumnRenamed(\"count\", \"Number of Employees\") \\\n",
    "           .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "216330b1-4254-4831-8557-616e2b282793",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Maximum and minimum salary in Engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff8b6155-6071-4f0a-ae1b-92b1e34fc358",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+--------------+\n| Department|Minimum Salary|Maximum Salary|\n+-----------+--------------+--------------+\n|         HR|         52000|         53000|\n|Engineering|         60000|         70000|\n|  Marketing|         45000|         48000|\n+-----------+--------------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import min, max\n",
    "employee_df.groupBy(employee_df.Department) \\\n",
    "           .agg(min(employee_df.Salary).alias(\"Minimum Salary\"), \n",
    "                max(employee_df.Salary).alias(\"Maximum Salary\")) \\\n",
    "           .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63fb3569-2aca-44d6-8b40-ccd25af1ca49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Join and Combine Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df97343b-b403-41c3-a1ef-77af77720788",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "4.  Perform an inner join between employee_data and performance_data on Name ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ac87351-74c3-4701-9ae5-f29397a14877",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------+----+------+\n|  Name| Department|Salary|Year|Rating|\n+------+-----------+------+----+------+\n|Ananya|         HR| 52000|2023|   4.5|\n|Fatima|  Marketing| 45000|2023|   3.9|\n| Karan|         HR| 53000|2023|   4.1|\n|Naveen|Engineering| 70000|2023|   4.7|\n| Priya|Engineering| 60000|2023|   4.3|\n| Rahul|Engineering| 65000|2023|   4.9|\n|  Zoya|  Marketing| 48000|2023|   3.8|\n+------+-----------+------+----+------+\n\n"
     ]
    }
   ],
   "source": [
    "employee_performance_df = employee_df.join(performance_df, on=\"Name\", how=\"inner\")\n",
    "employee_performance_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ea6da0b-3d0a-485a-a4b4-d72122c59ed1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "5. Show each employee’s salary and performance rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d1016c8-6322-44f2-bd35-1fd0a61aba67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+\n|  Name|Salary|Rating|\n+------+------+------+\n|Ananya| 52000|   4.5|\n|Fatima| 45000|   3.9|\n| Karan| 53000|   4.1|\n|Naveen| 70000|   4.7|\n| Priya| 60000|   4.3|\n| Rahul| 65000|   4.9|\n|  Zoya| 48000|   3.8|\n+------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "employee_performance_df.select(\"Name\", \"Salary\", \"Rating\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3484d846-88a1-4e32-93ae-c3e91e112b75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "6. Filter employees with rating > 4.5 and salary > 60000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd2f5580-9e21-45d5-bf0d-0083f37ae31d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------+----+------+\n|  Name| Department|Salary|Year|Rating|\n+------+-----------+------+----+------+\n|Naveen|Engineering| 70000|2023|   4.7|\n| Rahul|Engineering| 65000|2023|   4.9|\n+------+-----------+------+----+------+\n\n"
     ]
    }
   ],
   "source": [
    "employee_performance_df.filter((employee_performance_df.Rating > 4.5) & (employee_performance_df.Salary > 60000)) \\\n",
    "                       .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e463f747-1868-4af6-9fb7-7041d7950ae3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###  Window & Rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5717596b-2f13-4e7b-b07a-80d47fc90a04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "7. Rank employees by salary department-wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0c5e197-fd0f-4931-89be-94520ef77d8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------+----+\n|  Name| Department|Salary|Rank|\n+------+-----------+------+----+\n|Naveen|Engineering| 70000|   1|\n| Rahul|Engineering| 65000|   2|\n| Priya|Engineering| 60000|   3|\n| Karan|         HR| 53000|   1|\n|Ananya|         HR| 52000|   2|\n|  Zoya|  Marketing| 48000|   1|\n|Fatima|  Marketing| 45000|   2|\n+------+-----------+------+----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank, desc\n",
    "\n",
    "dept_df = Window.partitionBy(\"Department\") \\\n",
    "                .orderBy(desc(\"Salary\"))\n",
    "employee_df = employee_df.withColumn(\"Rank\", rank().over(dept_df))\n",
    "employee_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4432daf-bd8d-415d-a40d-2db2ad8af2db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "8. Calculate cumulative salary in each department."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8c29c8a-30d2-4242-9284-8a956a52237c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------+-----------------+\n|  Name| Department|Salary|Cumulative_Salary|\n+------+-----------+------+-----------------+\n| Priya|Engineering| 60000|            60000|\n| Rahul|Engineering| 65000|           125000|\n|Naveen|Engineering| 70000|           195000|\n|Ananya|         HR| 52000|            52000|\n| Karan|         HR| 53000|           105000|\n|Fatima|  Marketing| 45000|            45000|\n|  Zoya|  Marketing| 48000|            93000|\n+------+-----------+------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "dept_df = Window.partitionBy(\"Department\") \\\n",
    "                    .orderBy(\"Salary\") \\\n",
    "                    .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "cumm_df = employee_df.withColumn(\"Cumulative_Salary\", sum(\"Salary\").over(dept_df))\n",
    "\n",
    "cumm_df.select(\"Name\", \"Department\", \"Salary\", \"Cumulative_Salary\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17b6ae4f-1a62-47c3-b075-8f9f3914381a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Date Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23fd45dc-cf70-4a84-9fe8-e5b5cdccff78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "9.Add a new column JoinDate with random dates between 2020 and 2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f91dcac-8ef8-4b96-8c38-521b0989a07f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------+----+----------+\n|  Name| Department|Salary|Rank|  JoinDate|\n+------+-----------+------+----+----------+\n|Naveen|Engineering| 70000|   1|2023-06-14|\n| Rahul|Engineering| 65000|   2|2021-01-13|\n| Priya|Engineering| 60000|   3|2021-03-01|\n| Karan|         HR| 53000|   1|2023-03-30|\n|Ananya|         HR| 52000|   2|2020-08-11|\n|  Zoya|  Marketing| 48000|   1|2020-01-15|\n|Fatima|  Marketing| 45000|   2|2023-03-02|\n+------+-----------+------+----+----------+\n\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DateType\n",
    "def random_date():\n",
    "    start = datetime(2020, 1, 1)\n",
    "    end = datetime(2023, 12, 31)\n",
    "    return start + timedelta(days=random.randint(0, (end - start).days))\n",
    "\n",
    "random_date_udf = udf(random_date, DateType())\n",
    "employee_df = employee_df.withColumn(\"JoinDate\", random_date_udf())\n",
    "employee_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e76e8f0b-3064-4190-a2b3-d427c4baaf4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "10. Add column YearsWithCompany using current_date() and datediff() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9ee1206-105b-43fd-b053-e8d665dfb82e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------+----+----------+----------------+\n|  Name| Department|Salary|Rank|  JoinDate|YearsWithCompany|\n+------+-----------+------+----+----------+----------------+\n|Naveen|Engineering| 70000|   1|2023-01-14|               2|\n| Rahul|Engineering| 65000|   2|2020-04-18|               5|\n| Priya|Engineering| 60000|   3|2021-10-20|               3|\n| Karan|         HR| 53000|   1|2021-02-17|               4|\n|Ananya|         HR| 52000|   2|2023-03-05|               2|\n|  Zoya|  Marketing| 48000|   1|2020-03-13|               5|\n|Fatima|  Marketing| 45000|   2|2023-06-27|               1|\n+------+-----------+------+----+----------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import current_date, datediff, floor\n",
    "\n",
    "employee_df = employee_df.withColumn(\"YearsWithCompany\",\n",
    "                                    floor(datediff(current_date(), employee_df.JoinDate) / 365)\n",
    "                                    )\n",
    "\n",
    "employee_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1b25458-0bb0-4e04-820e-ccce056bac2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Writing to Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0307e2d4-8d4d-489c-bcff-2f037830d46b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "11. Write the full employee DataFrame to CSV with headers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a075d7c-d1d3-4a37-80a1-46144ec6be89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "employee_df.write.mode(\"overwrite\").option(\"header\", True).csv(\"dbfs:/FileStore/performance_csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "939f9147-2419-470f-8b5e-6a36794f480f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "12. Save the joined DataFrame to a Parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ba31659-11f5-4e42-bbe3-cc6d87d65b16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "employee_performance_df.write.mode(\"overwrite\").parquet(\"dbfs:/FileStore/joined_data_parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f9979a5e-5590-4530-a27d-326b7160f321",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Displaying  csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "850b3588-3188-432f-8a0d-0c142c581df4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/FileStore/employee_data_csv/</td><td>employee_data_csv/</td><td>0</td><td>1749632743000</td></tr><tr><td>dbfs:/FileStore/joined_data_parquet/</td><td>joined_data_parquet/</td><td>0</td><td>1749632828000</td></tr><tr><td>dbfs:/FileStore/performance_csv/</td><td>performance_csv/</td><td>0</td><td>1749633311000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/FileStore/employee_data_csv/",
         "employee_data_csv/",
         0,
         1749632743000
        ],
        [
         "dbfs:/FileStore/joined_data_parquet/",
         "joined_data_parquet/",
         0,
         1749632828000
        ],
        [
         "dbfs:/FileStore/performance_csv/",
         "performance_csv/",
         0,
         1749633311000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs ls dbfs:/FileStore/"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6580995690131375,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Aggregation and Grouping",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}