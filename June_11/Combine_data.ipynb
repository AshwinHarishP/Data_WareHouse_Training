{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a042996-0101-4367-a592-217966e167b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=1053308873284391#setting/sparkui/0611-085717-4ds1crvb/driver-1107440672652021086\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*, 4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7244234fccd0>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "                    .appName(\"CombineData\") \\\n",
    "                    .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef4f699c-e669-4c3d-b15d-0f70ee80cf80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Reading CSV files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "948259e0-b59d-4cad-a686-60d010431f85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Dataset: Combine Existing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e85ef4e5-c645-4f62-9779-7b85bea7811f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "employee_data = [ \n",
    "    (\"Ananya\", \"HR\", 52000), \n",
    "    (\"Rahul\", \"Engineering\", 65000), \n",
    "    (\"Priya\", \"Engineering\", 60000), \n",
    "    (\"Zoya\", \"Marketing\", 48000), \n",
    "    (\"Karan\", \"HR\", 53000), \n",
    "    (\"Naveen\", \"Engineering\", 70000), \n",
    "    (\"Fatima\", \"Marketing\", 45000) \n",
    "]\n",
    "employee_columns = [\"Name\", \"Department\", \"Salary\"]\n",
    "employee_df = spark.createDataFrame(employee_data, employee_columns)\n",
    "\n",
    "performance_data = [ \n",
    "    (\"Ananya\", 2023, 4.5), \n",
    "    (\"Rahul\", 2023, 4.9), \n",
    "    (\"Priya\", 2023, 4.3), \n",
    "    (\"Zoya\", 2023, 3.8), \n",
    "    (\"Karan\", 2023, 4.1), \n",
    "    (\"Naveen\", 2023, 4.7), \n",
    "    (\"Fatima\", 2023, 3.9) \n",
    "]\n",
    "performance_column = [\"Name\", \"Year\", \"Rating\"]\n",
    "performance_df = spark.createDataFrame(performance_data, performance_column)\n",
    "\n",
    "# Project Data\n",
    "project_data = [ \n",
    "    (\"Ananya\", \"HR Portal\", 120), \n",
    "    (\"Rahul\", \"Data Platform\", 200), \n",
    "    (\"Priya\", \"Data Platform\", 180), \n",
    "    (\"Zoya\", \"Campaign Tracker\", 100), \n",
    "    (\"Karan\", \"HR Portal\", 130), \n",
    "    (\"Naveen\", \"ML Pipeline\", 220), \n",
    "    (\"Fatima\", \"Campaign Tracker\", 90) \n",
    "]\n",
    "project_column = [\"Name\", \"Project\", \"HoursWorked\"]\n",
    "project_df = spark.createDataFrame(project_data, project_column)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9dd94d9-1cb4-4c16-9bd1-8ca25ae275af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Dataset 3: project_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7a18ab4-5de1-4887-8a94-f95f563c9fc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###  PySpark Exercises â€“ Set 3 (Project, Nulls, Functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72932b9e-4faf-4753-a8b1-50fdf86dfae9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Joins and Advanced Aggregations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e07001a-21a5-482d-88f0-494394f6b473",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Join employee_data, performance_data and project _data ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9dc95dc6-9218-4f4c-a211-f4fb410a223a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------+----+------+----------------+-----------+\n|  Name| Department|Salary|Year|Rating|         Project|HoursWorked|\n+------+-----------+------+----+------+----------------+-----------+\n|Ananya|         HR| 52000|2023|   4.5|       HR Portal|        120|\n| Rahul|Engineering| 65000|2023|   4.9|   Data Platform|        200|\n| Priya|Engineering| 60000|2023|   4.3|   Data Platform|        180|\n|  Zoya|  Marketing| 48000|2023|   3.8|Campaign Tracker|        100|\n| Karan|         HR| 53000|2023|   4.1|       HR Portal|        130|\n|Naveen|Engineering| 70000|2023|   4.7|     ML Pipeline|        220|\n|Fatima|  Marketing| 45000|2023|   3.9|Campaign Tracker|         90|\n+------+-----------+------+----+------+----------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "emp_perf_df = employee_df.join(performance_df, on=\"Name\", how=\"inner\")\n",
    "full_df = emp_perf_df.join(project_df, on=\"Name\", how=\"inner\")\n",
    "\n",
    "full_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bf8dfaa-47c0-49d4-89ab-1f7926586417",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Compute total hours worked per department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5c908dd-9a62-4985-93ab-0f94088de1c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+\n| Department|TotalHoursWorked|\n+-----------+----------------+\n|         HR|             250|\n|Engineering|             600|\n|  Marketing|             190|\n+-----------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "full_df.groupBy(\"Department\") \\\n",
    "                 .agg(sum(\"HoursWorked\").alias(\"TotalHoursWorked\")) \\\n",
    "                 .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f23ebe1b-5573-4f67-8292-7c599d3a386d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Compute average rating per project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "085804cd-e372-4004-aa37-6be074f15580",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------+\n|         Project|AverageRating|\n+----------------+-------------+\n|       HR Portal|          4.3|\n|   Data Platform|          4.6|\n|Campaign Tracker|         3.85|\n|     ML Pipeline|          4.7|\n+----------------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg, round\n",
    "average_rating = full_df.groupBy(\"Project\") \\\n",
    "                        .agg(round(avg(\"Rating\"),2).alias(\"AverageRating\"))\n",
    "\n",
    "average_rating.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49cf32e3-e978-4045-8c39-efcf1b62b2da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Handling Missing Data (introduce some manually)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96361158-a97b-42e0-87f9-b32f083ffcc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "4. Add a row to performance_data with a None rating ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf692a4c-0efd-4ece-8413-b2248618f7de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----+------+\n|         Name|Year|Rating|\n+-------------+----+------+\n|       Ananya|2023|   4.5|\n|        Rahul|2023|   4.9|\n|        Priya|2023|   4.3|\n|         Zoya|2023|   3.8|\n|        Karan|2023|   4.1|\n|       Naveen|2023|   4.7|\n|       Fatima|2023|   3.9|\n|Ashwin Harish|2024|  NULL|\n+-------------+----+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "new_data = [(\"Ashwin Harish\", 2024, None)]\n",
    "new_data_df = spark.createDataFrame(new_data, schema=performance_df.schema)\n",
    "\n",
    "\n",
    "performance_df = performance_df.union(new_data_df)\n",
    "\n",
    "performance_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6fdf650-ba87-40c8-a694-f9958b453c7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "5. Filter rows with null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b2ec00a-9101-42a5-bf14-d0415dfd65a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----+------+\n|         Name|Year|Rating|\n+-------------+----+------+\n|Ashwin Harish|2024|  NULL|\n+-------------+----+------+\n\n"
     ]
    }
   ],
   "source": [
    "performance_df.filter(performance_df.Rating.isNull()) \\\n",
    "              .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7fd1a76-fba0-4555-85fa-576b30b4b7e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "6. Replace null ratings with the department average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b15ab981-c53d-4bf0-b65b-11d264d28ebf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+------+----+------+----------------+-----------+\n| Department|  Name|Salary|Year|Rating|         Project|HoursWorked|\n+-----------+------+------+----+------+----------------+-----------+\n|         HR|Ananya| 52000|2023|   4.5|       HR Portal|        120|\n|Engineering| Rahul| 65000|2023|   4.9|   Data Platform|        200|\n|Engineering| Priya| 60000|2023|   4.3|   Data Platform|        180|\n|         HR| Karan| 53000|2023|   4.1|       HR Portal|        130|\n|  Marketing|  Zoya| 48000|2023|   3.8|Campaign Tracker|        100|\n|Engineering|Naveen| 70000|2023|   4.7|     ML Pipeline|        220|\n|  Marketing|Fatima| 45000|2023|   3.9|Campaign Tracker|         90|\n+-----------+------+------+----+------+----------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "dept_avg = full_df.groupBy(\"Department\") \\\n",
    "                  .agg(avg(\"Rating\").alias(\"DeptAvgRating\"))\n",
    "\n",
    "full_df_with_avg = full_df.join(dept_avg, on=\"Department\", how=\"left\")\n",
    "\n",
    "performance_df = full_df_with_avg.withColumn(\"Rating\",\n",
    "                                            when(col(\"Rating\").isNull(), col(\"DeptAvgRating\")).otherwise(col(\"Rating\"))) \\\n",
    "                                            .drop(\"DeptAvgRating\")\n",
    "\n",
    "performance_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b793ff1-2542-4f87-9d06-68aa20135b88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####  Built-In Functions and UDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8382bbf3-be8b-4097-91de-e6ddf01b9641",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "7. Create a column \\\n",
    "PerformanceCategory : \\\n",
    " Excellent (>=4.7), \\\n",
    " Good (4.0â€“4.69), \\\n",
    " Average (<4.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fde4000e-cd14-422a-8f90-b16985a6a30f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+------+----+------+----------------+-----------+-------------------+\n| Department|  Name|Salary|Year|Rating|         Project|HoursWorked|PerformanceCategory|\n+-----------+------+------+----+------+----------------+-----------+-------------------+\n|         HR|Ananya| 52000|2023|   4.5|       HR Portal|        120|               Good|\n|Engineering| Rahul| 65000|2023|   4.9|   Data Platform|        200|          Excellent|\n|Engineering| Priya| 60000|2023|   4.3|   Data Platform|        180|               Good|\n|  Marketing|  Zoya| 48000|2023|   3.8|Campaign Tracker|        100|            Average|\n|         HR| Karan| 53000|2023|   4.1|       HR Portal|        130|               Good|\n|Engineering|Naveen| 70000|2023|   4.7|     ML Pipeline|        220|          Excellent|\n|  Marketing|Fatima| 45000|2023|   3.9|Campaign Tracker|         90|            Average|\n+-----------+------+------+----+------+----------------+-----------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "performance_df = performance_df.withColumn(\"PerformanceCategory\", \n",
    "                                           when(performance_df.Rating >= 4.7, \"Excellent\")\n",
    "                                           .when((performance_df.Rating >= 4.0) & (performance_df.Rating <= 4.69), \"Good\")\n",
    "                                           .otherwise(\"Average\")\n",
    "                                           )\n",
    "performance_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4e57d48-bddc-4cf9-a596-6bd3e3be1f39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Create a UDF to assign bonus based on HoursWorked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64244213-1889-4124-b990-12b618d320d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+------+----+------+----------------+-----------+-------------------+-----+\n| Department|  Name|Salary|Year|Rating|         Project|HoursWorked|PerformanceCategory|Bonus|\n+-----------+------+------+----+------+----------------+-----------+-------------------+-----+\n|         HR|Ananya| 52000|2023|   4.5|       HR Portal|        120|               Good| 5000|\n|Engineering| Priya| 60000|2023|   4.3|   Data Platform|        180|               Good| 5000|\n|Engineering| Rahul| 65000|2023|   4.9|   Data Platform|        200|          Excellent| 5000|\n|  Marketing|  Zoya| 48000|2023|   3.8|Campaign Tracker|        100|            Average| 5000|\n|         HR| Karan| 53000|2023|   4.1|       HR Portal|        130|               Good| 5000|\n|Engineering|Naveen| 70000|2023|   4.7|     ML Pipeline|        220|          Excellent|10000|\n|  Marketing|Fatima| 45000|2023|   3.9|Campaign Tracker|         90|            Average| 5000|\n+-----------+------+------+----+------+----------------+-----------+-------------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "def bonus_udf(hours):\n",
    "    if hours is None:\n",
    "        return 0 \n",
    "    return 10000 if hours > 200 else 5000\n",
    "\n",
    "bonus = udf(bonus_udf, IntegerType())\n",
    "\n",
    "performance_df = performance_df.withColumn(\"Bonus\", bonus(col(\"HoursWorked\")))\n",
    "performance_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "498157b1-273d-40ad-856f-4d6b769d2308",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Date and Time Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16e4d1b2-b1f3-42fa-8c77-6c7c809026e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "9. Add a column JoinDate with 2021-06-01 for all, then add \n",
    "difference from today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3de6936-6d22-40e6-8a5e-23680e9e80eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------+----------+------------------+\n|  Name| Department|Salary|  JoinDate|Difference_in_days|\n+------+-----------+------+----------+------------------+\n|Ananya|         HR| 52000|2021-06-01|              1471|\n| Rahul|Engineering| 65000|2021-06-01|              1471|\n| Priya|Engineering| 60000|2021-06-01|              1471|\n|  Zoya|  Marketing| 48000|2021-06-01|              1471|\n| Karan|         HR| 53000|2021-06-01|              1471|\n|Naveen|Engineering| 70000|2021-06-01|              1471|\n|Fatima|  Marketing| 45000|2021-06-01|              1471|\n+------+-----------+------+----------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit, to_date, date_diff, current_date\n",
    "\n",
    "employee_df = employee_df.withColumn(\"JoinDate\", to_date(lit(\"2021-06-01\")))\n",
    "employee_df = employee_df.withColumn(\"Difference_in_days\", date_diff(current_date(), employee_df.JoinDate).alias('date_diff'))\n",
    "employee_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed6dbc6c-d050-4b7d-9068-3d5b02b2c853",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "10. Calculate how many employees joined before 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6673df6f-096d-4654-8e84-cf26c0d57007",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of employees joined before 2022: 7\n"
     ]
    }
   ],
   "source": [
    "employee_count = employee_df.filter(col(\"JoinDate\") < \"2022-01-01\").count()\n",
    "print(f\"Number of employees joined before 2022: {employee_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93150fc1-b7ab-4259-b51e-9a230a04f56c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Unions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8597eafc-583a-4e0f-ae15-f3046b365ade",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "11. create another small team DataFrame and union() it with employee_data .\n",
    "\n",
    "extra_employees = \\\n",
    "[ \n",
    "(\"Meena\", \"HR\", 48000), \n",
    "(\"Raj\", \"Marketing\", 51000) \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79489c08-4e69-4afb-97ea-e5fdc27fdb0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------+----------+------------------+\n|  Name| Department|Salary|  JoinDate|Difference_in_days|\n+------+-----------+------+----------+------------------+\n|Ananya|         HR| 52000|2021-06-01|              1471|\n| Rahul|Engineering| 65000|2021-06-01|              1471|\n| Priya|Engineering| 60000|2021-06-01|              1471|\n|  Zoya|  Marketing| 48000|2021-06-01|              1471|\n| Karan|         HR| 53000|2021-06-01|              1471|\n|Naveen|Engineering| 70000|2021-06-01|              1471|\n|Fatima|  Marketing| 45000|2021-06-01|              1471|\n| Meena|         HR| 48000|2021-06-01|              1471|\n|   Raj|  Marketing| 51000|2021-06-01|              1471|\n+------+-----------+------+----------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "new_employees_data = [\n",
    "    (\"Meena\", \"HR\", 48000, \"2021-06-01\", 1471),\n",
    "    (\"Raj\", \"Marketing\", 51000, \"2021-06-01\", 1471)\n",
    "]\n",
    "\n",
    "new_employees_columns = [\"Name\", \"Department\", \"Salary\", \"JoinDate\", \"Difference_in_days\"]\n",
    "\n",
    "new_employees_df = spark.createDataFrame(new_employees_data, new_employees_columns)\n",
    "\n",
    "combined_employee_df = employee_df.union(new_employees_df)\n",
    "\n",
    "combined_employee_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f0bf7f9-f508-4982-a10f-af2c253bcf8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Saving Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00b4079e-9788-4cb8-ad5f-22009fa6d285",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "12. save the final merged dataset (all 3 joins) as a partitioned parquet file based\n",
    "on Department ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f47c983-2564-4af2-8a80-8e2436a8887b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "full_df.write \\\n",
    "    .partitionBy(\"Department\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(\"dbfs:/FileStore/joined_data_parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a719f95-1127-40c0-aeda-ec744bea3fbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6580995690131425,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Combine_data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}