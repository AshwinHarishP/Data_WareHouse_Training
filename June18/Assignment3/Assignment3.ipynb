{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Creating spark session"
      ],
      "metadata": {
        "id": "gl880g7nkJQZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "rgp3nFKWkGU6",
        "outputId": "067c6377-7cca-46b8-948e-679b1d85ea8d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7c41f8763dd0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://fcf4b08e0038:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Retail Transactions</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "                    .appName(\"Retail Transactions\") \\\n",
        "                    .getOrCreate()\n",
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data Ingestion & Schema Handling\n"
      ],
      "metadata": {
        "id": "CoAOSaLoke_M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Load the CSV using inferred schema.\n"
      ],
      "metadata": {
        "id": "gLNIP1_Cks14"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "employee_df = spark.read.format('csv').option(\"header\", True) \\\n",
        "                                      .option(\"inferSchema\", True) \\\n",
        "                                      .load('/content/drive/MyDrive/Assignment/Employee_Timesheet.csv')\n",
        "\n",
        "employee_df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DzAxxbXnkNyC",
        "outputId": "665c6f04-173c-4f1e-bb13-9c1fab898c57"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "root\n",
            " |-- EmployeeID: string (nullable = true)\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Department: string (nullable = true)\n",
            " |-- Project: string (nullable = true)\n",
            " |-- WorkHours: integer (nullable = true)\n",
            " |-- WorkDate: date (nullable = true)\n",
            " |-- Location: string (nullable = true)\n",
            " |-- Mode: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Load the same file with schema explicitly defined.\n"
      ],
      "metadata": {
        "id": "LehimEuylZid"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType\n",
        "\n",
        "employee_schema = StructType([\n",
        "    StructField(\"EmployeeID\", StringType(), True),\n",
        "    StructField(\"Name\", StringType(), True),\n",
        "    StructField(\"Department\", StringType(), True),\n",
        "    StructField(\"Project\", StringType(), True),\n",
        "    StructField(\"WorkHours\", IntegerType(), True),\n",
        "    StructField(\"WorkDate\", DateType(), True),\n",
        "    StructField(\"Location\", StringType(), True),\n",
        "    StructField(\"Mode\", StringType(), True)\n",
        "])\n",
        "\n",
        "employee_custom_df = spark.read.format('csv').option(\"header\", True) \\\n",
        "                                      .schema(employee_schema) \\\n",
        "                                      .load('/content/drive/MyDrive/Assignment/Employee_Timesheet.csv')\n",
        "\n",
        "employee_custom_df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fHysq-lla6_",
        "outputId": "162f7bfd-37d8-4145-deb7-4e4583b466fb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- EmployeeID: string (nullable = true)\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Department: string (nullable = true)\n",
            " |-- Project: string (nullable = true)\n",
            " |-- WorkHours: integer (nullable = true)\n",
            " |-- WorkDate: date (nullable = true)\n",
            " |-- Location: string (nullable = true)\n",
            " |-- Mode: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Add a new column Weekday extracted from WorkDate"
      ],
      "metadata": {
        "id": "JfOK1oU8l-Y8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import date_format, col\n",
        "\n",
        "employee_custom_df = employee_custom_df.withColumn(\"WeekDay\", date_format(col(\"WorkDate\"), \"EEEE\"))\n",
        "employee_custom_df.select(\n",
        "                            'EmployeeID',\n",
        "                            'Name',\n",
        "                            'Department',\n",
        "                            'Project',\n",
        "                            'WorkDate',\n",
        "                            'WeekDay'\n",
        ").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UOzVEBnvl9td",
        "outputId": "3e6bdeac-8152-4d6a-8732-d9fb57471b4c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+----------+-------+----------+---------+\n",
            "|EmployeeID| Name|Department|Project|  WorkDate|  WeekDay|\n",
            "+----------+-----+----------+-------+----------+---------+\n",
            "|      E101|Anita|        IT|  Alpha|2024-05-01|Wednesday|\n",
            "|      E102|  Raj|        HR|   Beta|2024-05-01|Wednesday|\n",
            "|      E103| John|   Finance|  Alpha|2024-05-02| Thursday|\n",
            "|      E101|Anita|        IT|  Alpha|2024-05-03|   Friday|\n",
            "|      E104|Meena|        IT|  Gamma|2024-05-03|   Friday|\n",
            "|      E102|  Raj|        HR|   Beta|2024-05-04| Saturday|\n",
            "+----------+-----+----------+-------+----------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  Aggregations & Grouping"
      ],
      "metadata": {
        "id": "Fn6VPH3ynDBu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Calculate total work hours by employee.\n"
      ],
      "metadata": {
        "id": "0i8KxW1BnFIn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import sum\n",
        "total_hours_employee = employee_custom_df.groupBy('EmployeeID') \\\n",
        "                                        .agg(sum('WorkHours').alias('TotalWorkHours')) \\\n",
        "                                        .orderBy('TotalWorkHours', ascending=False)\n",
        "total_hours_employee.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0fcQBOHnEBB",
        "outputId": "753bd227-2d07-4d1a-ef2f-69a6b1068d95"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------------+\n",
            "|EmployeeID|TotalWorkHours|\n",
            "+----------+--------------+\n",
            "|      E101|            17|\n",
            "|      E102|            15|\n",
            "|      E104|             6|\n",
            "|      E103|             5|\n",
            "+----------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " 5. Calculate average work hours per department."
      ],
      "metadata": {
        "id": "N68B1EAOn7fo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import avg, round\n",
        "employee_custom_df.groupBy('Department') \\\n",
        "                  .agg(round(avg('WorkHours'),2).alias('AverageWorkHours')) \\\n",
        "                  .show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrbyWpj8n8Tu",
        "outputId": "8a438444-c369-4a90-9a48-826b57f7e766"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------------+\n",
            "|Department|AverageWorkHours|\n",
            "+----------+----------------+\n",
            "|        HR|             7.5|\n",
            "|   Finance|             5.0|\n",
            "|        IT|            7.67|\n",
            "+----------+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " 6. Get top 2 employees by total hours using window function.\n"
      ],
      "metadata": {
        "id": "Kcm7kst3oGsB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import row_number, desc, col\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "window_spec = Window.orderBy(desc('TotalWorkHours'))\n",
        "\n",
        "total_hours_employee.withColumn('Rank', row_number().over(window_spec)) \\\n",
        "                                      .filter(col('Rank') <= 2) \\\n",
        "                                      .show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vh29Rr5VoHou",
        "outputId": "ce834961-c387-41ea-d053-c3fc6b1b7b54"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------------+----+\n",
            "|EmployeeID|TotalWorkHours|Rank|\n",
            "+----------+--------------+----+\n",
            "|      E101|            17|   1|\n",
            "|      E102|            15|   2|\n",
            "+----------+--------------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Date Operations"
      ],
      "metadata": {
        "id": "wLEVMb0ibNbJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Filter entries where WorkDate falls on a weekend"
      ],
      "metadata": {
        "id": "vjrVCeUGbOY9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import dayofweek\n",
        "employee_custom_df.filter(dayofweek(col('WorkDate')).isin(1, 7))   \\\n",
        "                  .select(\n",
        "                            'EmployeeID',\n",
        "                            'Name',\n",
        "                            'WorkDate',\n",
        "                            'WeekDay'\n",
        "                          ).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yk5BzpOvbJ20",
        "outputId": "0f0ac251-236b-4ea7-b2a3-cd15af76ad33"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----+----------+--------+\n",
            "|EmployeeID|Name|  WorkDate| WeekDay|\n",
            "+----------+----+----------+--------+\n",
            "|      E102| Raj|2024-05-04|Saturday|\n",
            "+----------+----+----------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Calculate running total of hours per employee using window.\n"
      ],
      "metadata": {
        "id": "LHBipE31bhoN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import sum\n",
        "window_emp = Window.partitionBy('EmployeeID').orderBy('WorkDate') \\\n",
        "                   .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
        "\n",
        "\n",
        "employee_custom_df.withColumn('RunningTotal', sum('WorkHours').over(window_emp)) \\\n",
        "                  .select(\n",
        "                            'EmployeeID',\n",
        "                            'Name',\n",
        "                            'WorkDate',\n",
        "                            'WorkHours',\n",
        "                            'RunningTotal') \\\n",
        "                  .show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLlq9MDnbipw",
        "outputId": "19cc08a4-2614-43ed-a247-86a6da23c6cc"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+----------+---------+------------+\n",
            "|EmployeeID| Name|  WorkDate|WorkHours|RunningTotal|\n",
            "+----------+-----+----------+---------+------------+\n",
            "|      E101|Anita|2024-05-01|        8|           8|\n",
            "|      E101|Anita|2024-05-03|        9|          17|\n",
            "|      E102|  Raj|2024-05-01|        7|           7|\n",
            "|      E102|  Raj|2024-05-04|        8|          15|\n",
            "|      E103| John|2024-05-02|        5|           5|\n",
            "|      E104|Meena|2024-05-03|        6|           6|\n",
            "+----------+-----+----------+---------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  Joining DataFrames\n"
      ],
      "metadata": {
        "id": "-JOwHr2ccfHu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Create department_location.csv"
      ],
      "metadata": {
        "id": "pwn1UE6Ici1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dept_data = [\n",
        "              ('IT', 'Anand'),\n",
        "              ('HR', 'Shruti'),\n",
        "              ('Finance', 'Kamal')\n",
        "          ]\n",
        "\n",
        "dept_columns = ['Department', 'DeptHead']\n",
        "dept_df = spark.createDataFrame(dept_data, dept_columns)\n",
        "dept_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVK5ClONcIpW",
        "outputId": "33298f3e-ab46-4c7e-c5c3-2b708c08fec1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------+\n",
            "|Department|DeptHead|\n",
            "+----------+--------+\n",
            "|        IT|   Anand|\n",
            "|        HR|  Shruti|\n",
            "|   Finance|   Kamal|\n",
            "+----------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Join with timesheet data and list all employees with their DeptHead.\n"
      ],
      "metadata": {
        "id": "dfoAM2dZc0Oi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import first\n",
        "\n",
        "employee_with_head = employee_custom_df.join(dept_df, on='Department', how='left')\n",
        "\n",
        "employee_with_head.groupBy('EmployeeID').agg(\n",
        "                                              first('Name').alias('Name'),\n",
        "                                              first('Department').alias('Department'),\n",
        "                                              first('DeptHead').alias('DeptHead')\n",
        "                                             ).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pM0cOiTQc1HK",
        "outputId": "df084953-7b8e-4b57-f2ad-c0f788d37b0c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+----------+--------+\n",
            "|EmployeeID| Name|Department|DeptHead|\n",
            "+----------+-----+----------+--------+\n",
            "|      E101|Anita|        IT|   Anand|\n",
            "|      E102|  Raj|        HR|  Shruti|\n",
            "|      E103| John|   Finance|   Kamal|\n",
            "|      E104|Meena|        IT|   Anand|\n",
            "+----------+-----+----------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  Pivot & Unpivot"
      ],
      "metadata": {
        "id": "l2AWVpL_ePKu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Pivot table: total hours per employee per project.\n"
      ],
      "metadata": {
        "id": "7GUlRNAMeQJx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "employee_custom_df.groupBy('EmployeeID') \\\n",
        "                  .pivot('Project') \\\n",
        "                  .agg(sum('WorkHours')) \\\n",
        "                  .show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8-S7w8eeRpk",
        "outputId": "30f3bdfc-8f6b-40f4-d062-39b71f9b5136"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+----+-----+\n",
            "|EmployeeID|Alpha|Beta|Gamma|\n",
            "+----------+-----+----+-----+\n",
            "|      E103|    5|NULL| NULL|\n",
            "|      E104| NULL|NULL|    6|\n",
            "|      E101|   17|NULL| NULL|\n",
            "|      E102| NULL|  15| NULL|\n",
            "+----------+-----+----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. Unpivot example: Convert mode-specific hours into rows.\n"
      ],
      "metadata": {
        "id": "Hic03zoGelFm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import explode, array, struct, lit, coalesce\n",
        "\n",
        "mode_hours_df = employee_custom_df.groupBy('EmployeeID', 'Mode') \\\n",
        "                                  .agg(sum('WorkHours') \\\n",
        "                                  .alias('ModeHours'))\n",
        "\n",
        "mode_pivot_df = mode_hours_df.groupBy('EmployeeID') \\\n",
        "                             .pivot('Mode') \\\n",
        "                             .sum('ModeHours')\n",
        "\n",
        "unpivot_expr = explode(array(\n",
        "                              struct(lit('Onsite').alias('Mode'), col('Onsite').alias('ModeHours')),\n",
        "                              struct(lit('Remote').alias('Mode'), col('Remote').alias('ModeHours'))\n",
        "))\n",
        "\n",
        "mode_pivot_df.select('EmployeeID', unpivot_expr.alias('kv')) \\\n",
        "             .select('EmployeeID',\n",
        "                     col('kv.Mode'),\n",
        "                     coalesce(col('kv.ModeHours'), lit(0)).alias('ModeHours')) \\\n",
        "             .show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7N20YwYyfP0T",
        "outputId": "ddcea60d-2c74-4954-812e-b84619a88ec7"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------+---------+\n",
            "|EmployeeID|  Mode|ModeHours|\n",
            "+----------+------+---------+\n",
            "|      E103|Onsite|        0|\n",
            "|      E103|Remote|        5|\n",
            "|      E104|Onsite|        6|\n",
            "|      E104|Remote|        0|\n",
            "|      E101|Onsite|        0|\n",
            "|      E101|Remote|       17|\n",
            "|      E102|Onsite|        7|\n",
            "|      E102|Remote|        8|\n",
            "+----------+------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  UDF & Conditional Logic"
      ],
      "metadata": {
        "id": "9d-S2ymcgfCe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. Create a UDF to classify work hours:"
      ],
      "metadata": {
        "id": "TIxXcBk8ghGx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "def workload_tag(hours):\n",
        "    if hours >= 8:\n",
        "        return \"Full\"\n",
        "    elif hours >= 4:\n",
        "        return \"Partial\"\n",
        "    else:\n",
        "        return \"Light\"\n",
        "\n",
        "workload_udf = udf(workload_tag, StringType())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcE8QIXKgnvs",
        "outputId": "978b8ec0-5f49-4525-83ba-ac87896c0820"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+----------+---------+----------------+\n",
            "|EmployeeID| Name|  WorkDate|WorkHours|WorkloadCategory|\n",
            "+----------+-----+----------+---------+----------------+\n",
            "|      E102|  Raj|2024-05-01|        7|         Partial|\n",
            "|      E103| John|2024-05-02|        5|         Partial|\n",
            "|      E101|Anita|2024-05-01|        8|            Full|\n",
            "|      E102|  Raj|2024-05-04|        8|            Full|\n",
            "|      E101|Anita|2024-05-03|        9|            Full|\n",
            "|      E104|Meena|2024-05-03|        6|         Partial|\n",
            "+----------+-----+----------+---------+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. Add a column\n",
        "WorkloadCategory using this UDF."
      ],
      "metadata": {
        "id": "6PhaSKGHg7_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "employee_custom_df = employee_custom_df.withColumn(\"WorkloadCategory\", workload_udf(col(\"WorkHours\")))\n",
        "\n",
        "employee_custom_df.select(\n",
        "                            'EmployeeID',\n",
        "                            'Name',\n",
        "                            'WorkDate',\n",
        "                            'WorkHours',\n",
        "                            'WorkloadCategory'\n",
        ").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-0I9YXDg-IZ",
        "outputId": "a10c2820-4c5a-4bc7-a4f0-4a55f6c2be59"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+----------+---------+----------------+\n",
            "|EmployeeID| Name|  WorkDate|WorkHours|WorkloadCategory|\n",
            "+----------+-----+----------+---------+----------------+\n",
            "|      E102|  Raj|2024-05-01|        7|         Partial|\n",
            "|      E103| John|2024-05-02|        5|         Partial|\n",
            "|      E101|Anita|2024-05-01|        8|            Full|\n",
            "|      E102|  Raj|2024-05-04|        8|            Full|\n",
            "|      E101|Anita|2024-05-03|        9|            Full|\n",
            "|      E104|Meena|2024-05-03|        6|         Partial|\n",
            "+----------+-----+----------+---------+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  Nulls and Cleanup"
      ],
      "metadata": {
        "id": "y82rmGtYhAS9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. Introduce some nulls in Mode column.\n"
      ],
      "metadata": {
        "id": "ACMLs4u_hHui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import when\n",
        "employee_custom_df = employee_custom_df.withColumn(\"Mode\",\n",
        "                          when(col(\"EmployeeID\") == \"E101\", None).otherwise(col(\"Mode\")))\n",
        "\n",
        "employee_custom_df.select('EmployeeID', 'Mode').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HJQDqMxhBPn",
        "outputId": "8db80c45-14cd-4da8-f8b2-4580626a10eb"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------+\n",
            "|EmployeeID|  Mode|\n",
            "+----------+------+\n",
            "|      E102|Onsite|\n",
            "|      E103|Remote|\n",
            "|      E101|  NULL|\n",
            "|      E102|Remote|\n",
            "|      E101|  NULL|\n",
            "|      E104|Onsite|\n",
            "+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. Fill nulls with \"Not Provided\".\n"
      ],
      "metadata": {
        "id": "tH1k5rjqhbmP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "employee_custom_df = employee_custom_df.fillna({\"Mode\": \"Not Provided\"})\n",
        "\n",
        "employee_custom_df.select('EmployeeID', 'Mode').show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmQqLoj4hdeH",
        "outputId": "6f68ec13-8c9c-419d-f3d3-bbabbdb972f7"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------+\n",
            "|EmployeeID|        Mode|\n",
            "+----------+------------+\n",
            "|      E102|      Onsite|\n",
            "|      E103|      Remote|\n",
            "|      E101|Not Provided|\n",
            "|      E102|      Remote|\n",
            "|      E101|Not Provided|\n",
            "|      E104|      Onsite|\n",
            "+----------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. Drop rows where WorkHours < 4."
      ],
      "metadata": {
        "id": "SlSzZbughSIP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "employee_custom_df = employee_custom_df.filter(col(\"WorkHours\") >= 4)\n",
        "\n",
        "employee_custom_df.select('EmployeeID', 'WorkHours').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqZjnH-Ahmu7",
        "outputId": "4fdbee22-6515-431e-a06c-d06a5921a74e"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---------+\n",
            "|EmployeeID|WorkHours|\n",
            "+----------+---------+\n",
            "|      E102|        7|\n",
            "|      E103|        5|\n",
            "|      E101|        8|\n",
            "|      E102|        8|\n",
            "|      E101|        9|\n",
            "|      E104|        6|\n",
            "+----------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  Advanced Conditions"
      ],
      "metadata": {
        "id": "lquCuhxKhyHV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. Use when-otherwise to mark employees as \"Remote Worker\" if >80% entries are Remote."
      ],
      "metadata": {
        "id": "KDK37OuArrfw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import count, when\n",
        "\n",
        "remote_ratio_df = employee_custom_df.groupBy('EmployeeID') \\\n",
        "                                    .agg(count(\"*\").alias('TotalEntries'),\n",
        "                                         count(when(col('Mode') == 'Remote', True)).alias('RemoteCount'))\\\n",
        "                                    .withColumn('RemoteRatio', col('RemoteCount') / col('TotalEntries')) \\\n",
        "                                    .withColumn('WorkType', when(col('RemoteRatio') > 0.8, 'Remote Worker') \\\n",
        "                                                            .otherwise('Hybrid/Office'))\n",
        "\n",
        "remote_ratio_df.select(\n",
        "                       'EmployeeID',\n",
        "                       'RemoteRatio',\n",
        "                       'WorkType'\n",
        "                       ).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CeQTPDIOr0Y_",
        "outputId": "d02fde27-65a5-4173-b91f-437c4a58ba36"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+-------------+\n",
            "|EmployeeID|RemoteRatio|     WorkType|\n",
            "+----------+-----------+-------------+\n",
            "|      E103|        1.0|Remote Worker|\n",
            "|      E104|        0.0|Hybrid/Office|\n",
            "|      E101|        0.0|Hybrid/Office|\n",
            "|      E102|        0.5|Hybrid/Office|\n",
            "+----------+-----------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. Add a new column\n",
        "ExtraHours where hours > 8."
      ],
      "metadata": {
        "id": "fTInTb5fsoHz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "employee_custom_df = employee_custom_df.withColumn('ExtraHours',\n",
        "                                                   when(col('WorkHours') > 8, col('WorkHours') - 8) \\\n",
        "                                                   .otherwise(0))\n",
        "\n",
        "employee_custom_df.select(\n",
        "                          'EmployeeID',\n",
        "                          'WorkHours',\n",
        "                          'ExtraHours'\n",
        "                          ).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9tjlhEnsoyB",
        "outputId": "88f2ce05-4f24-4d22-d258-6232469f1fbe"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---------+----------+\n",
            "|EmployeeID|WorkHours|ExtraHours|\n",
            "+----------+---------+----------+\n",
            "|      E102|        7|         0|\n",
            "|      E103|        5|         0|\n",
            "|      E101|        8|         0|\n",
            "|      E102|        8|         0|\n",
            "|      E101|        9|         1|\n",
            "|      E104|        6|         0|\n",
            "+----------+---------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Union + Duplicate Handling"
      ],
      "metadata": {
        "id": "9TLq-BN9tGpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. Append a dummy timesheet for new interns using unionByName()\n"
      ],
      "metadata": {
        "id": "DcyJ0sSvtJRl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import to_date, col\n",
        "\n",
        "sample_data = [\n",
        "                (\"I001\", \"Intern1\", \"IT\", \"ProjX\", 6, \"2024-06-10\", \"Office\", \"Office\"),\n",
        "                (\"I002\", \"Intern2\", \"HR\", \"ProjY\", 5, \"2024-06-11\", \"Remote\", \"Remote\")\n",
        "              ]\n",
        "\n",
        "sample_columns = ['EmployeeID', 'Name', 'Department', 'Project', 'WorkHours', 'WorkDate', 'WorkMode', 'ActualWorkMode']\n",
        "\n",
        "sample_df = spark.createDataFrame(sample_data, sample_columns) \\\n",
        "                 .withColumn(\"WorkDate\", to_date(col(\"WorkDate\")))\n",
        "\n",
        "combined_df = employee_custom_df.unionByName(sample_df, allowMissingColumns=True) # Setting Nulls for missing columns\n",
        "\n",
        "combined_df.select(\n",
        "                    'EmployeeID',\n",
        "                    'Name',\n",
        "                    'Department',\n",
        "                    'WorkHours',\n",
        "                    'WorkDate'\n",
        "                   ).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umfng-2RtKpG",
        "outputId": "cc8e0511-837e-4509-9e7d-2a0c9cc11ce9"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------+----------+---------+----------+\n",
            "|EmployeeID|   Name|Department|WorkHours|  WorkDate|\n",
            "+----------+-------+----------+---------+----------+\n",
            "|      E102|    Raj|        HR|        7|2024-05-01|\n",
            "|      E103|   John|   Finance|        5|2024-05-02|\n",
            "|      E101|  Anita|        IT|        8|2024-05-01|\n",
            "|      E102|    Raj|        HR|        8|2024-05-04|\n",
            "|      E101|  Anita|        IT|        9|2024-05-03|\n",
            "|      E104|  Meena|        IT|        6|2024-05-03|\n",
            "|      I001|Intern1|        IT|        6|2024-06-10|\n",
            "|      I002|Intern2|        HR|        5|2024-06-11|\n",
            "+----------+-------+----------+---------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. Remove duplicate rows based on all columns."
      ],
      "metadata": {
        "id": "hlZ7FDrNu1if"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "combined_df = combined_df.unionAll(combined_df)\n",
        "combined_df.show()\n",
        "\n",
        "combined_df = combined_df.dropDuplicates()\n",
        "combined_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPTx66Z_twl6",
        "outputId": "fecf40eb-d0bb-4a63-c3a8-50748e2a1c54"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------+----------+-------+---------+----------+---------+------------+---------+----------------+----------+--------+--------------+\n",
            "|EmployeeID|   Name|Department|Project|WorkHours|  WorkDate| Location|        Mode|  WeekDay|WorkloadCategory|ExtraHours|WorkMode|ActualWorkMode|\n",
            "+----------+-------+----------+-------+---------+----------+---------+------------+---------+----------------+----------+--------+--------------+\n",
            "|      E104|  Meena|        IT|  Gamma|        6|2024-05-03|Hyderabad|      Onsite|   Friday|         Partial|         0|    NULL|          NULL|\n",
            "|      E102|    Raj|        HR|   Beta|        8|2024-05-04|   Mumbai|      Remote| Saturday|            Full|         0|    NULL|          NULL|\n",
            "|      E103|   John|   Finance|  Alpha|        5|2024-05-02|    Delhi|      Remote| Thursday|         Partial|         0|    NULL|          NULL|\n",
            "|      E101|  Anita|        IT|  Alpha|        9|2024-05-03|Bangalore|Not Provided|   Friday|            Full|         1|    NULL|          NULL|\n",
            "|      E101|  Anita|        IT|  Alpha|        8|2024-05-01|Bangalore|Not Provided|Wednesday|            Full|         0|    NULL|          NULL|\n",
            "|      E102|    Raj|        HR|   Beta|        7|2024-05-01|   Mumbai|      Onsite|Wednesday|         Partial|         0|    NULL|          NULL|\n",
            "|      I001|Intern1|        IT|  ProjX|        6|2024-06-10|     NULL|        NULL|     NULL|            NULL|      NULL|  Office|        Office|\n",
            "|      I002|Intern2|        HR|  ProjY|        5|2024-06-11|     NULL|        NULL|     NULL|            NULL|      NULL|  Remote|        Remote|\n",
            "|      E104|  Meena|        IT|  Gamma|        6|2024-05-03|Hyderabad|      Onsite|   Friday|         Partial|         0|    NULL|          NULL|\n",
            "|      E102|    Raj|        HR|   Beta|        8|2024-05-04|   Mumbai|      Remote| Saturday|            Full|         0|    NULL|          NULL|\n",
            "|      E103|   John|   Finance|  Alpha|        5|2024-05-02|    Delhi|      Remote| Thursday|         Partial|         0|    NULL|          NULL|\n",
            "|      E101|  Anita|        IT|  Alpha|        9|2024-05-03|Bangalore|Not Provided|   Friday|            Full|         1|    NULL|          NULL|\n",
            "|      E101|  Anita|        IT|  Alpha|        8|2024-05-01|Bangalore|Not Provided|Wednesday|            Full|         0|    NULL|          NULL|\n",
            "|      E102|    Raj|        HR|   Beta|        7|2024-05-01|   Mumbai|      Onsite|Wednesday|         Partial|         0|    NULL|          NULL|\n",
            "|      I001|Intern1|        IT|  ProjX|        6|2024-06-10|     NULL|        NULL|     NULL|            NULL|      NULL|  Office|        Office|\n",
            "|      I002|Intern2|        HR|  ProjY|        5|2024-06-11|     NULL|        NULL|     NULL|            NULL|      NULL|  Remote|        Remote|\n",
            "|      E104|  Meena|        IT|  Gamma|        6|2024-05-03|Hyderabad|      Onsite|   Friday|         Partial|         0|    NULL|          NULL|\n",
            "|      E102|    Raj|        HR|   Beta|        8|2024-05-04|   Mumbai|      Remote| Saturday|            Full|         0|    NULL|          NULL|\n",
            "|      E103|   John|   Finance|  Alpha|        5|2024-05-02|    Delhi|      Remote| Thursday|         Partial|         0|    NULL|          NULL|\n",
            "|      E101|  Anita|        IT|  Alpha|        9|2024-05-03|Bangalore|Not Provided|   Friday|            Full|         1|    NULL|          NULL|\n",
            "+----------+-------+----------+-------+---------+----------+---------+------------+---------+----------------+----------+--------+--------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+----------+-------+----------+-------+---------+----------+---------+------------+---------+----------------+----------+--------+--------------+\n",
            "|EmployeeID|   Name|Department|Project|WorkHours|  WorkDate| Location|        Mode|  WeekDay|WorkloadCategory|ExtraHours|WorkMode|ActualWorkMode|\n",
            "+----------+-------+----------+-------+---------+----------+---------+------------+---------+----------------+----------+--------+--------------+\n",
            "|      E104|  Meena|        IT|  Gamma|        6|2024-05-03|Hyderabad|      Onsite|   Friday|         Partial|         0|    NULL|          NULL|\n",
            "|      E102|    Raj|        HR|   Beta|        8|2024-05-04|   Mumbai|      Remote| Saturday|            Full|         0|    NULL|          NULL|\n",
            "|      E103|   John|   Finance|  Alpha|        5|2024-05-02|    Delhi|      Remote| Thursday|         Partial|         0|    NULL|          NULL|\n",
            "|      E101|  Anita|        IT|  Alpha|        9|2024-05-03|Bangalore|Not Provided|   Friday|            Full|         1|    NULL|          NULL|\n",
            "|      E101|  Anita|        IT|  Alpha|        8|2024-05-01|Bangalore|Not Provided|Wednesday|            Full|         0|    NULL|          NULL|\n",
            "|      E102|    Raj|        HR|   Beta|        7|2024-05-01|   Mumbai|      Onsite|Wednesday|         Partial|         0|    NULL|          NULL|\n",
            "|      I001|Intern1|        IT|  ProjX|        6|2024-06-10|     NULL|        NULL|     NULL|            NULL|      NULL|  Office|        Office|\n",
            "|      I002|Intern2|        HR|  ProjY|        5|2024-06-11|     NULL|        NULL|     NULL|            NULL|      NULL|  Remote|        Remote|\n",
            "+----------+-------+----------+-------+---------+----------+---------+------------+---------+----------------+----------+--------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving to parquet file"
      ],
      "metadata": {
        "id": "DLAM-F9ivZUM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "combined_df.write.mode(\"overwrite\") \\\n",
        "                 .parquet(\"/content/drive/MyDrive/Assignment/Transformed_Employee_Timesheet\")"
      ],
      "metadata": {
        "id": "yTA9aqlgvale"
      },
      "execution_count": 53,
      "outputs": []
    }
  ]
}